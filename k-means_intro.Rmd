---
title: "k-means explanation"
output: html_notebook
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.width = 9)
```
#### **Intro**

It all began with the fact that I've decided to study k-means clustering algorithm. The more resources I read, the more confused . The  definition of cluster centers has been stumbling-block. The most accessible explanation was provided by [Wikipedia](https://en.wikipedia.org/wiki/K-means_clustering):

> Assign each observation to the cluster whose mean has the least squared Euclidean distance.

But how does this happen? So I've decided to reinvent a wheel and reproduce step by step all stages of the algorithm.
As a basis I've decided to take an example from the book by Roger D. Peng [Exploratory Data Analysis with R](https://bookdown.org/rdpeng/exdata/k-means-clustering.html).

Because I'm a passionate fan of R I've decided to use fantastic package [tydiverse](https://www.tidyverse.org/) to reproduce algorithm and [gganimate](https://github.com/thomasp85/gganimate) to visualize result.  
So, let's begin!

#### **Dataset preparing**
I've decided to reproduce dataset that is provided by Roger D.Peng (Figure 12.1: Simulated dataset)![](simulated_dataset.png)
```{r}
set.seed(1234)
x <- c(0.75, 1.1, 1.25, 0.55, 2.15, 2.17, 1.85, 1.86, 2.87, 2.75, 2.91, 2.71)
y <- c(0.6, 1.0, 1.25, 1.0, 1.9, 1.8, 1.85, 2.5, 1.0, 0.85, 0.86, 1.18)
df_xy <- tibble(x, y)
# Length of dataset
n_xy <- dim(df_xy)[1]
```
Let's take three clusters for example
```{r}
Cx = c(1, 1.7, 2.5)
Cy = c(2, 1.0, 1.5)
# Number of clusters define as length of Cx or Cy vectors
n_clust <- length(Cx)
df_cl <- tibble(Cluster = factor(seq(n_clust), ordered = T), Cx, Cy)
```
Next create a dataset with coordinates of observations and coordinates of cluster centers
```{r}
df_kmeans0 <- bind_cols(df_xy, map_dfr(df_cl, rep, each = length(x) / n_clust))
```
Here's how all this looks like at the initial stage
```{r}
ggplot(df_kmeans0, aes(x, y)) +
  geom_point(shape = 1, size = 5, stroke = 1) +
  geom_text(aes(x, y, label = rownames(df_xy)), vjust = 2.0, size = 4, color = "black") + 
  geom_point(aes(Cx, Cy, color = Cluster), shape = 3, size = 5, stroke = 1) +
  labs(title = "The initial location of the cluster centers") +
  theme(plot.title = element_text(hjust = 0.5))
```
Now let's look what's happen under the hood of k-means

#### **Step 1. Calculate squared Euclidean distance between each observations and each clusters**
Firs of all we need to choose a metric for measuring distance between observations and cluster's centers. Let it be Euclidean distance
or rather squared Euclidean distance.

> Eu_dist^2^ = (x~i~ - x~c~)^2^ + (y~i~ - y~c~)^2^

Next, it needs to determine the distance from each point to each cluster. Since we have 12 points and 3 clusters, we get 36 distances. It was the first stumbling-block.
```{r}
df_dist <- bind_cols(map_dfr(df_xy, rep, n_clust),
                     map_dfr(df_cl, rep, each = n_xy)) %>% 
  mutate(Eu_dist = sqrt((.$x - .$Cx)^2 + (.$y - .$Cy)^2))
df_dist
```
#### **Step.2 Assign each observation to the cluster whose mean has the least squared Euclidean distance**

```{r}
step2 <- df_dist %>% 
  select(-Cx, -Cy) %>% 
  spread(Cluster, Eu_dist) %>% 
  nest(-x, -y) %>%
  mutate(Eu_dist = map_dbl(.$data, min),
         Cluster = map_dbl(.$data, which.min) %>% factor(., ordered = T)) %>%
  unnest(data) %>% 
  arrange(match(x, df_xy$x)) # відновлюємо порядок слідування точок як у вихідному датасеті
step2
```
Columns 1-3 represent all (3 in our case)  distances of observation to each cluster. We need to choose the minimum value for each observation. They are gathered in Eu_dist column. So, the columns Eu_dist and Cluster demonstrate which observations to which clusters belong. It was the second and the last stumbling-block. 

#### **Step 3. Join cluster's coordinates to previous dataset**
Now it remains to add the coordinates of cluster centers to our previous data set
```{r}
step2 %>%
  left_join(df_cl, by = 'Cluster') -> step3
step3
```
Let's demonstrate result
```{r}
ggplot(step3, aes(x, y, color = Cluster)) +
  geom_point(size = 5) +
  geom_point(aes(x = step3$Cx, y = step3$Cy), shape = 3, size = 7) +
  geom_text(label = rownames(step3), vjust = 1.8, size = 4, color = 'black') 
```
#### **Step 4. Recalculation center's coordinates of new clusters**
```{r}
step3 %>% 
  select(-num_range("", 1:n_clust)) %>% 
  group_by(Cluster) %>% 
  mutate(Cx2 = mean(x),
         Cy2 = mean(y)) -> step4
step4
```
Let's look at new clusters (smaller size - are initial values)
```{r}
step4 %>% 
  ggplot(aes(x, y, color = Cluster)) +
    geom_point(size = 5) +
    geom_point(aes(x = step4$Cx, y = step4$Cy), shape = 3, size = 3) +
    geom_point(aes(x = step4$Cx2, y = step4$Cy2), shape = 3, size = 6) +
    geom_text(aes(label = rownames(step4)), vjust = 1.8, size = 4, color = 'black') 
```
Put our dataset with new clusters into "canonical" form
```{r}
step4 %>% 
  select(Cluster, Cx2, Cy2) %>% 
  group_by(Cluster) %>% 
  unique(.) %>% 
  arrange(Cluster) -> new_iteration
new_iteration
```
Next - repeat iteration from step 1 and so on, but for this purpose it needs to have something like recursive function. And this is stated further.
At this stage, I've became aware of the principle of k-means, but I wanted to see the results for several iterations and compare the results with a regular algorithm - the kmeans() function.
